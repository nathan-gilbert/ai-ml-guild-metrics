{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Metrics\n",
    "\n",
    "## Determining Model Accuracy\n",
    "\n",
    "It's very common to train a variety of models, apply each to held out sample and\n",
    "score the results. Sometimes, a third hold set is used to test a model on\n",
    "_completely_ new data. Typically, these 2 hold out sets are called the\n",
    "_validation_ and _test_ sets respectively. In order to evaluate a model, a\n",
    "suitable metric for the dataset needs to be selected.\n",
    "\n",
    "Datasets with rare occurring labellings can produce misleading model performance\n",
    "if a less nuanced metric is chosen to measure it.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Simple accuracy is the measure of how many discrete labelling by a classifier\n",
    "(or regressor) were correct."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rate (manually calculated): 0.750000\n",
      "Accuracy Rate (via accuracy_score()): 0.750000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simple synthetic data\n",
    "training_points = [[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]\n",
    "training_labels = [1, 1, 1, 2, 2, 2]\n",
    "X = np.array(training_points)\n",
    "Y = np.array(training_labels)\n",
    "\n",
    "# Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, Y)\n",
    "\n",
    "# Classify test data with the classifier\n",
    "test_points = [[1, 1], [2, 2], [3, 3], [4, 3]]\n",
    "test_labels = [2, 2, 2, 1]\n",
    "predicts = gnb.predict(test_points)\n",
    "\n",
    "count = len([True for idx, label in enumerate(test_labels) if label == predicts[idx]])\n",
    "print(\"Accuracy Rate (manually calculated): %f\" % (float(count) / len(test_labels)))\n",
    "print(\"Accuracy Rate (via accuracy_score()): %f\" % accuracy_score(test_labels, predicts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What about accuracy and cross validation?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "result = cross_val_score(gnb , X, Y, cv = kf)\n",
    "\n",
    "print(\"Avg accuracy: {}\".format(result.mean()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recall, Precision and F-Measure, Oh My!\n",
    "\n",
    "To better understand your model's effectiveness use the Recall, Precision and\n",
    "F-Measure metrics. The following are some definitions to help us better understand\n",
    "these metrics:\n",
    "\n",
    "- True Positive (TP): True positive represents the value of correct predictions of positives out of actual positive cases.\n",
    "- False Positive (FP): False positive represents the value of incorrect positive predictions.\n",
    "- True Negative (TN): True negative represents the value of correct predictions of negatives out of actual negative cases.\n",
    "- False Negative (FN): False negative represents the value of incorrect negative predictions.\n",
    "\n",
    "_Recall_ is the measure of how many correct labellings your model predicted. It\n",
    "is defined as: *Recall Score = TP / (FN + TP)*\n",
    "_Precision_ is the measure of how many incorrect labellings your model predicted.\n",
    "*Precision Score = TP / (FP + TP)*\n",
    "_F-measure_ is something like an average of the two scores.\n",
    "*F1 Score = 2 * Precision Score * Recall Score / (Precision Score + Recall Score)*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X = bc.data\n",
    "Y = bc.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1, stratify=Y)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# train the support vector machine\n",
    "svc = SVC(kernel='linear', C=10.0, random_state=1)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred))\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### ROC & AUC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem-Specific Metrics\n",
    "\n",
    "### ROUGE & BLEU\n",
    "\n",
    "### Clustering\n",
    "\n",
    "### Question Answering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}